{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BU6r7SVppdlE"
   },
   "source": [
    "# Amazon reviews for cell phones and accessories dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaGlzCRmqSET"
   },
   "source": [
    "### 3. **Word Embedding models**\n",
    "\n",
    "a. Learn word2vec models using gensim on this dataset with the following settings:\n",
    "(a) Size=100, 200, 300, (b) Window=3,7, (c) Min_count=2, 5. Use skipgram.\n",
    "i. This will give 12 word2vec models. For each of these models, for each\n",
    "review take average word embeddings and train a logistic regression.\n",
    "Report accuracy on test set.\n",
    "\n",
    "b. Use the already available google word2vec model. For each review take average word embeddings and train a logistic regression. Report accuracy on test set.\n",
    "\n",
    "c. Use the already available glove models: 50D, 100D and 200D. For each review\n",
    "take average word embeddings and train a logistic regression. Report accuracy on test set for each of the three sized embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "strQphfgqpmt"
   },
   "source": [
    "### 4. **Neural Network models**\n",
    "\n",
    "a. RNNs: Train a single directional RNN with L layers. Vary the number of layers\n",
    "(as 1,2,3,4) and also size of layers (20, 50, 100, 200). Report accuracy on test set.\n",
    "\n",
    "b. LSTMs: Train a single directional LSTM with L layers. Vary the number of\n",
    "layers (as 1,2,3,4) and also size of layers (20, 50, 100, 200). Report accuracy on test set.\n",
    "\n",
    "c. BiLSTM: Train a single directional RNN with L layers. Vary the number of\n",
    "layers (as 1,2,3,4) and also size of layers (20, 50, 100, 200). Report accuracy on test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pt82zp4pdWL"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ox0fiB1rBi9"
   },
   "source": [
    "## Connecting to drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2tL8kNM9wNKG",
    "outputId": "c8239d2e-c05d-44a0-f192-78d160499752"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/Drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/Drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BjpAnGKGzCZT",
    "outputId": "c4d7ba05-918a-43fb-d80f-99d8a4b40d2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "File directory: /content/drive/My Drive/midterm (1)/dataset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Mount your Google Drive to access files stored there\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Replace 'your_file_name.csv' with the actual name of your file.\n",
    "file_name = 'Cell_Phones_and_Accessories_5.json'\n",
    "\n",
    "# Set the root directory to your Google Drive\n",
    "root_dir = '/content/drive/My Drive/'\n",
    "\n",
    "# Function to recursively search for the file in all directories and subdirectories\n",
    "def find_file(directory):\n",
    "    for item in os.listdir(directory):\n",
    "        item_path = os.path.join(directory, item)\n",
    "        if os.path.isfile(item_path) and item == file_name:\n",
    "            return directory\n",
    "        elif os.path.isdir(item_path):\n",
    "            result = find_file(item_path)\n",
    "            if result:\n",
    "                return result\n",
    "    return None\n",
    "\n",
    "# Call the function to find the file directory\n",
    "file_directory = find_file(root_dir)\n",
    "\n",
    "# Print the file directory\n",
    "if file_directory:\n",
    "    print(\"File directory:\", file_directory)\n",
    "else:\n",
    "    print(\"File not found in Google Drive.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LesUZTqMzCLy"
   },
   "outputs": [],
   "source": [
    "os.chdir('/content/drive/My Drive/midterm (1)/dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WhdEutrzLf9"
   },
   "source": [
    "## Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "m5L7AVB-zCKS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "GURXgyy9zCFf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import json\n",
    "import re\n",
    "import multiprocessing as mp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2N_JyEKSzCA0"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUKzpzp7zS3Z"
   },
   "source": [
    "## Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6qTb15Z9zB8p"
   },
   "outputs": [],
   "source": [
    "# Step 1: Read the JSON file and convert it to DataFrame\n",
    "df = pd.read_json('Cell_Phones_and_Accessories_5.json', lines=True)\n",
    "\n",
    "# Now, 'df' contains the data in DataFrame format, and you can work with it as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "ANl3ael_zB6L",
    "outputId": "531b1585-2cf9-4e32-dee4-8ebb6e5aec90"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "  <div id=\"df-17267c74-02f1-4dde-a75d-589931a75d7e\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A30TL5EWN6DFXT</td>\n",
       "      <td>120401325X</td>\n",
       "      <td>christina</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>They look good and stick good! I just don't li...</td>\n",
       "      <td>4</td>\n",
       "      <td>Looks Good</td>\n",
       "      <td>1400630400</td>\n",
       "      <td>05 21, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ASY55RVNIL0UD</td>\n",
       "      <td>120401325X</td>\n",
       "      <td>emily l.</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>These stickers work like the review says they ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Really great product.</td>\n",
       "      <td>1389657600</td>\n",
       "      <td>01 14, 2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-17267c74-02f1-4dde-a75d-589931a75d7e')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "\n",
       "\n",
       "\n",
       "    <div id=\"df-86ac1f6a-cfed-4cb9-bbe8-badd66fdade0\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-86ac1f6a-cfed-4cb9-bbe8-badd66fdade0')\"\n",
       "              title=\"Suggest charts.\"\n",
       "              style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "    </div>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "    background-color: #E8F0FE;\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: #1967D2;\n",
       "    height: 32px;\n",
       "    padding: 0 0 0 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: #E2EBFA;\n",
       "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: #174EA6;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "    background-color: #3B4455;\n",
       "    fill: #D2E3FC;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart:hover {\n",
       "    background-color: #434B5C;\n",
       "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "    fill: #FFFFFF;\n",
       "  }\n",
       "</style>\n",
       "\n",
       "    <script>\n",
       "      async function quickchart(key) {\n",
       "        const containerElement = document.querySelector('#' + key);\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      }\n",
       "    </script>\n",
       "\n",
       "\n",
       "      <script>\n",
       "\n",
       "function displayQuickchartButton(domScope) {\n",
       "  let quickchartButtonEl =\n",
       "    domScope.querySelector('#df-86ac1f6a-cfed-4cb9-bbe8-badd66fdade0 button.colab-df-quickchart');\n",
       "  quickchartButtonEl.style.display =\n",
       "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "}\n",
       "\n",
       "        displayQuickchartButton(document);\n",
       "      </script>\n",
       "      <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-17267c74-02f1-4dde-a75d-589931a75d7e button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-17267c74-02f1-4dde-a75d-589931a75d7e');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "       reviewerID        asin reviewerName helpful  \\\n",
       "0  A30TL5EWN6DFXT  120401325X    christina  [0, 0]   \n",
       "1   ASY55RVNIL0UD  120401325X     emily l.  [0, 0]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  They look good and stick good! I just don't li...        4   \n",
       "1  These stickers work like the review says they ...        5   \n",
       "\n",
       "                 summary  unixReviewTime   reviewTime  \n",
       "0             Looks Good      1400630400  05 21, 2014  \n",
       "1  Really great product.      1389657600  01 14, 2014  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a_hs2X3czB4y",
    "outputId": "7b207f44-3460-480c-93cf-780db7b26ae7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 194439 entries, 0 to 194438\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count   Dtype \n",
      "---  ------          --------------   ----- \n",
      " 0   reviewerID      194439 non-null  object\n",
      " 1   asin            194439 non-null  object\n",
      " 2   reviewerName    190920 non-null  object\n",
      " 3   helpful         194439 non-null  object\n",
      " 4   reviewText      194439 non-null  object\n",
      " 5   overall         194439 non-null  int64 \n",
      " 6   summary         194439 non-null  object\n",
      " 7   unixReviewTime  194439 non-null  int64 \n",
      " 8   reviewTime      194439 non-null  object\n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 13.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "LqC9CsrhzB09"
   },
   "outputs": [],
   "source": [
    "# Extracting the 'reviewText' and 'overall' columns\n",
    "df = df[['reviewText', 'overall']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtXdFhsqzctd"
   },
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmlT9aUvzfjG"
   },
   "source": [
    "Sampling to make the data manageable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "TVwcgXuozBx7"
   },
   "outputs": [],
   "source": [
    "# The percentage of data to sample\n",
    "sample_percentage = 0.025\n",
    "\n",
    "# Perform simple random sampling\n",
    "sample_data = df.sample(frac=sample_percentage, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rSo925-zlqF"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAM20XHyzm8G"
   },
   "source": [
    "#### Cleaning the text column ('reviewText') by:\n",
    "1. Removing stop words\n",
    "2. Convert text to lowercase\n",
    "3. Removing punctuations and numbers\n",
    "4. Tokenizing\n",
    "5. Stemming and\n",
    "6. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pHNhceZbzBvm",
    "outputId": "78127838-dc6e-41f9-d014-536d27035d0c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "osLf4bdFzt1u"
   },
   "outputs": [],
   "source": [
    "# Define stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Define lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "\n",
    "    # Tokenize and remove stop words\n",
    "    tokenized_text = [w for w in word_tokenize(text) if w not in stop_words]\n",
    "    text = ' '.join(tokenized_text)\n",
    "\n",
    "    # Perform stemming and lemmatization\n",
    "    stemmed_lemmatized_text = [stemmer.stem(lemmatizer.lemmatize(w)) for w in word_tokenize(text)]\n",
    "    text = ' '.join(stemmed_lemmatized_text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UzUeFyuCzu1x",
    "outputId": "bdc7c9a7-8b47-4b5a-969a-88b35a37c0b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               reviewText  overall\n",
      "156187  ibolt xprodock activ car dock holder mount sam...        5\n",
      "102252  pouch everyth look otter box commut case aroun...        5\n",
      "23146   first case iphon previous one free one give al...        3\n",
      "86461   order case case htc inspir case last year abso...        5\n",
      "62407   bought gift big hit love choic color made devi...        5\n",
      "...                                                   ...      ...\n",
      "145652  work well samsung note ii charg charger rest p...        2\n",
      "165116  use brand extern batteri pack et np k charger ...        5\n",
      "65593   perfect case highli recommend samsung galaxi i...        5\n",
      "42927   got phone upgrad week swap need soon got home ...        5\n",
      "9887    lucki enough abl telecommut home full time spe...        5\n",
      "\n",
      "[4861 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "sample_data['reviewText'] = sample_data['reviewText'].apply(preprocess_text)\n",
    "print(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJkg7WbUz3dz"
   },
   "source": [
    "## Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "kSx8M0dszu0i"
   },
   "outputs": [],
   "source": [
    "# Split the data\n",
    "train_size = int(0.7 * len(sample_data))\n",
    "val_size = int(0.1 * len(sample_data))\n",
    "\n",
    "train_data = sample_data[:train_size]\n",
    "val_data = sample_data[train_size : train_size+val_size]\n",
    "test_data = sample_data[train_size+val_size:]\n",
    "\n",
    "# Extract the 'reviewText' and 'overall' fields\n",
    "X_train, y_train = train_data['reviewText'], train_data['overall']\n",
    "X_val, y_val = val_data['reviewText'], val_data['overall']\n",
    "X_test, y_test = test_data['reviewText'], test_data['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rDhdf1nuzuzS",
    "outputId": "436a7eb1-a48e-4304-c1ce-37b81a9c4095"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3402,), (3402,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train.shape), (y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rEHB7TqszurW",
    "outputId": "9ee1596f-8916-4d2f-8359-1138ae0ab719"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((486,), (486,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_val.shape), (y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FnOABwRbzuoo",
    "outputId": "9a264dc8-30bd-4d82-c079-b45976a9113e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((973,), (973,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_test.shape), (y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hvb2Xhpb0Dcg"
   },
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8-cFPsGrOZu"
   },
   "source": [
    "3. **Average of Word Embeddings:**\n",
    "   - **Description:** This approach computes the average word embeddings of each review and uses them as features to train classifiers like logistic regression.\n",
    "   - **Advantages:** Simple, captures context, useful for short texts.\n",
    "   - **Disadvantages:** Ignores word order, loses some semantic information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiqrdVbJ0IJa"
   },
   "source": [
    "a. **word2vec models using gensim**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "4GQNizO2zum9"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "4jmXR8G5zukZ"
   },
   "outputs": [],
   "source": [
    "# Function to extract word embeddings\n",
    "def extract_word_embeddings(reviews, w2v_model):\n",
    "    embeddings = []\n",
    "    for tokens in reviews:\n",
    "        embeddings_sum = np.zeros(w2v_model.vector_size)\n",
    "        word_count = 0\n",
    "        for token in tokens:\n",
    "            if token in w2v_model.wv:\n",
    "                embeddings_sum += w2v_model.wv[token]\n",
    "                word_count += 1\n",
    "        if word_count > 0:\n",
    "            embeddings.append(embeddings_sum / word_count)\n",
    "        else:\n",
    "            embeddings.append(np.zeros(w2v_model.vector_size))\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "fXejclIIzuhm"
   },
   "outputs": [],
   "source": [
    "# Train word2vec models and perform sentiment analysis with multiple ML models\n",
    "sizes = [100, 200, 300]\n",
    "windows = [3, 7]\n",
    "min_counts = [2, 5]\n",
    "ml_models = {\n",
    "    \"Logistic Regression\": LogisticRegression()\n",
    "}\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-PV4vMk0UnK"
   },
   "source": [
    "**word2vec models using gensim\n",
    "(a) Size=100, 200, 300, (b) Window=3,7, (c) Min_count=2, 5. Use skipgram.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XuZRpdgzud4",
    "outputId": "8ac299fe-cba2-4b62-d6a6-9b058337fd19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training word2vec_size_100_window_3_min_count_2...\n",
      "Training and evaluating Logistic Regression...\n",
      "Logistic Regression Accuracy: 0.5992\n",
      "Training word2vec_size_100_window_3_min_count_5...\n",
      "Training and evaluating Logistic Regression...\n",
      "Logistic Regression Accuracy: 0.6002\n",
      "Training word2vec_size_100_window_7_min_count_2...\n",
      "Training and evaluating Logistic Regression...\n",
      "Logistic Regression Accuracy: 0.6023\n",
      "Training word2vec_size_100_window_7_min_count_5...\n",
      "Training and evaluating Logistic Regression...\n",
      "Logistic Regression Accuracy: 0.6002\n",
      "Training word2vec_size_200_window_3_min_count_2...\n",
      "Training and evaluating Logistic Regression...\n",
      "Logistic Regression Accuracy: 0.5992\n",
      "Training word2vec_size_200_window_3_min_count_5...\n",
      "Training and evaluating Logistic Regression...\n",
      "Logistic Regression Accuracy: 0.5971\n",
      "Training word2vec_size_200_window_7_min_count_2...\n",
      "Training and evaluating Logistic Regression...\n",
      "Logistic Regression Accuracy: 0.6053\n",
      "Training word2vec_size_200_window_7_min_count_5...\n",
      "Training and evaluating Logistic Regression...\n",
      "Logistic Regression Accuracy: 0.6053\n",
      "Training word2vec_size_300_window_3_min_count_2...\n",
      "Training and evaluating Logistic Regression...\n",
      "Logistic Regression Accuracy: 0.5992\n",
      "Training word2vec_size_300_window_3_min_count_5...\n",
      "Training and evaluating Logistic Regression...\n",
      "Logistic Regression Accuracy: 0.5961\n",
      "Training word2vec_size_300_window_7_min_count_2...\n",
      "Training and evaluating Logistic Regression...\n",
      "Logistic Regression Accuracy: 0.6023\n",
      "Training word2vec_size_300_window_7_min_count_5...\n",
      "Training and evaluating Logistic Regression...\n",
      "Logistic Regression Accuracy: 0.5992\n"
     ]
    }
   ],
   "source": [
    "for size in sizes:\n",
    "    for window in windows:\n",
    "        for min_count in min_counts:\n",
    "            model_name = f\"word2vec_size_{size}_window_{window}_min_count_{min_count}\"\n",
    "            print(f\"Training {model_name}...\")\n",
    "\n",
    "            # Tokenize the cleaned text\n",
    "            tokenized_reviews = train_data['reviewText'].apply(word_tokenize)\n",
    "\n",
    "            # Train Word2Vec model\n",
    "            w2v_model = Word2Vec(tokenized_reviews, vector_size=size, window=window, min_count=min_count, sg=1)\n",
    "\n",
    "            # Extract word embeddings\n",
    "            X_train = extract_word_embeddings(tokenized_reviews, w2v_model)\n",
    "            y_train = train_data['overall'].values\n",
    "\n",
    "            # Evaluate with multiple ML models\n",
    "            for model_name, model in ml_models.items():\n",
    "                print(f\"Training and evaluating {model_name}...\")\n",
    "\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                # Extract word embeddings for test set\n",
    "                tokenized_test_reviews = test_data['reviewText'].apply(word_tokenize)\n",
    "                X_test = extract_word_embeddings(tokenized_test_reviews, w2v_model)\n",
    "                y_test = test_data['overall'].values\n",
    "\n",
    "                # Predict and calculate accuracy\n",
    "                y_pred = model.predict(X_test)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "                results[(model_name, model_name)] = accuracy\n",
    "\n",
    "                print(f\"{model_name} Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAC7RQnSsVZ2"
   },
   "source": [
    "The best performing model is word2vec_size_200_window_7_min_count_2\n",
    "\n",
    "Accuracy: 0.6053\n",
    "\n",
    "word2vec_size_200_window_7_min_count_5\n",
    "\n",
    "Accuracy: 0.6053"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6On6cqVu0esZ"
   },
   "source": [
    "b. **google word2vec model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ymtu_poBzuY-"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-1TSi1R0i8D",
    "outputId": "3ff8273c-2ded-4a9f-98b0-39a5e328ee83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================================-] 100.0% 1662.6/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# Download and load the Google News Word2Vec model\n",
    "# This will download the model if not already available in your Colab environment\n",
    "w2v_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "I0u7GAeh0i4w"
   },
   "outputs": [],
   "source": [
    "w2v_model_google= w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p6Zu_6fh0itZ",
    "outputId": "0ec406a7-ba79-45c8-c8cc-eca2715b93bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.6043\n"
     ]
    }
   ],
   "source": [
    "# Function to extract average word embeddings for each review\n",
    "def extract_average_embeddings(reviews, w2v_model):\n",
    "    embeddings = []\n",
    "    for review in reviews:\n",
    "        tokens = word_tokenize(review)\n",
    "        embeddings_sum = np.zeros(w2v_model.vector_size)\n",
    "        word_count = 0\n",
    "        for token in tokens:\n",
    "            if token in w2v_model:\n",
    "                embeddings_sum += w2v_model[token]\n",
    "                word_count += 1\n",
    "        if word_count > 0:\n",
    "            embeddings.append(embeddings_sum / word_count)\n",
    "        else:\n",
    "            embeddings.append(np.zeros(w2v_model.vector_size))\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Extract average embeddings for training data\n",
    "X_train = extract_average_embeddings(train_data['reviewText'], w2v_model)\n",
    "y_train = train_data['overall'].values\n",
    "\n",
    "# Extract average embeddings for test data\n",
    "X_test = extract_average_embeddings(test_data['reviewText'], w2v_model)\n",
    "y_test = test_data['overall'].values\n",
    "\n",
    "# Train logistic regression\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and calculate accuracy on the test set\n",
    "y_pred = lr_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Logistic Regression Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUtk1UCE0tbS"
   },
   "source": [
    "c. **Glove**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HYAWOrf00tw4",
    "outputId": "7f2f052a-0d4b-4ae6-9651-764de0f2288a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
      "[==================================================] 100.0% 252.1/252.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "#Load pre-trained GloVe models with different dimensions\n",
    "glove_50d = api.load(\"glove-wiki-gigaword-50\")\n",
    "glove_100d = api.load(\"glove-wiki-gigaword-100\")\n",
    "glove_200d = api.load(\"glove-wiki-gigaword-200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "0lJKnQow0xRp"
   },
   "outputs": [],
   "source": [
    "# Function to extract average word embeddings for each review\n",
    "def extract_average_embeddings(reviews, glove_model):\n",
    "    embeddings = []\n",
    "    for review in reviews:\n",
    "        tokens = word_tokenize(review)\n",
    "        embeddings_sum = np.zeros(glove_model.vector_size)\n",
    "        word_count = 0\n",
    "        for token in tokens:\n",
    "            if token in glove_model:\n",
    "                embeddings_sum += glove_model[token]\n",
    "                word_count += 1\n",
    "        if word_count > 0:\n",
    "            embeddings.append(embeddings_sum / word_count)\n",
    "        else:\n",
    "            embeddings.append(np.zeros(glove_model.vector_size))\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZPE-592-ur1"
   },
   "source": [
    "glove_50d model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GHX-gwB10xN6",
    "outputId": "8f46e1e3-7cf7-4da2-c7b7-7ddf37e38364"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.5848\n"
     ]
    }
   ],
   "source": [
    "# Extract average embeddings for training data\n",
    "X_train = extract_average_embeddings(train_data['reviewText'], glove_50d)\n",
    "y_train = train_data['overall'].values\n",
    "\n",
    "# Extract average embeddings for test data\n",
    "X_test = extract_average_embeddings(test_data['reviewText'], glove_50d)\n",
    "y_test = test_data['overall'].values\n",
    "\n",
    "# Train logistic regression\n",
    "lr_glove_50d_model = LogisticRegression()\n",
    "lr_glove_50d_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and calculate accuracy on the test set\n",
    "y_pred = lr_glove_50d_model.predict(X_test)\n",
    "accuracy_glove_50d = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_glove_50d:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCw-2c2o_cH4"
   },
   "source": [
    "glove_100d model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6UA8CxWF0xG6",
    "outputId": "3123efc6-3be6-4d35-8151-ec67639aa776"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.5848\n"
     ]
    }
   ],
   "source": [
    "# Extract average embeddings for training data\n",
    "X_train = extract_average_embeddings(train_data['reviewText'], glove_100d)\n",
    "y_train = train_data['overall'].values\n",
    "\n",
    "# Extract average embeddings for test data\n",
    "X_test = extract_average_embeddings(test_data['reviewText'], glove_100d)\n",
    "y_test = test_data['overall'].values\n",
    "\n",
    "# Train logistic regression\n",
    "lr_glove_100d_model = LogisticRegression()\n",
    "lr_glove_100d_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and calculate accuracy on the test set\n",
    "y_pred = lr_glove_100d_model.predict(X_test)\n",
    "accuracy_glove_100d = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_glove_100d:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8nFcWjq_d4I"
   },
   "source": [
    "glove_200d model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PYkNzBDv0xD0",
    "outputId": "febc6aae-bba3-4dbf-d3e5-baea4b4d15e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.5838\n"
     ]
    }
   ],
   "source": [
    "# Extract average embeddings for training data\n",
    "X_train = extract_average_embeddings(train_data['reviewText'], glove_200d)\n",
    "y_train = train_data['overall'].values\n",
    "\n",
    "# Extract average embeddings for test data\n",
    "X_test = extract_average_embeddings(test_data['reviewText'], glove_200d)\n",
    "y_test = test_data['overall'].values\n",
    "\n",
    "# Train logistic regression\n",
    "lr_glove_200d_model = LogisticRegression()\n",
    "lr_glove_200d_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and calculate accuracy on the test set\n",
    "y_pred = lr_glove_200d_model.predict(X_test)\n",
    "accuracy_glove_200d = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_glove_200d:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JpvHpTfk0w6m",
    "outputId": "1ab6f64c-a5b8-4a47-b753-e67637a0d67f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (50D): 0.5848\n",
      "Accuracy (100D): 0.5848\n",
      "Accuracy (200D): 0.5838\n"
     ]
    }
   ],
   "source": [
    "# make it in a format to get the below\n",
    "print(f\"Accuracy (50D): {accuracy_glove_50d:.4f}\")\n",
    "print(f\"Accuracy (100D): {accuracy_glove_100d:.4f}\")\n",
    "print(f\"Accuracy (200D): {accuracy_glove_200d:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjrWacCksz8z"
   },
   "source": [
    "Almost all the models peform equally well, with golve 50d and 100d having the highest acccuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGV4jrEStCkH"
   },
   "source": [
    "**Of all the word embeddings models the best performing model is word2vec model using genism with size=200, window=7, min_count= 2,5 and Accuracy: 0.6053**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJUulDby1HjY"
   },
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0yczpAyrapB"
   },
   "source": [
    "\n",
    "4. **Recurrent Neural Networks (RNNs):**\n",
    "   - **Description:** RNNs process sequences by maintaining hidden states that capture context. Used for various tasks including sequence generation and classification.\n",
    "   - **Advantages:** Captures sequential dependencies, flexible for various sequence lengths.\n",
    "   - **Disadvantages:** Struggles with long-range dependencies, vanishing/exploding gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wO2LeMz2ujkX"
   },
   "source": [
    "Steps for RNN models:\n",
    "\n",
    "1. Preprocess\n",
    "2. Tokenize the text\n",
    "3. Padding\n",
    "4. One Hot encodding\n",
    "5. Function for RNN\n",
    "6. RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "JaZiIfMN1HNP"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EN-2949_1TGv"
   },
   "source": [
    "tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "VNkgmY4r1HJ9"
   },
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "train_tokens = train_data['reviewText'].apply(word_tokenize)\n",
    "test_tokens = test_data['reviewText'].apply(word_tokenize)\n",
    "\n",
    "# Convert tokens to sequences of indices based on a vocabulary\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(train_tokens)\n",
    "X_train = tokenizer.texts_to_sequences(train_tokens)\n",
    "X_test = tokenizer.texts_to_sequences(test_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-voJCbb1XO5"
   },
   "source": [
    "padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "3DNyPqZX1HIG"
   },
   "outputs": [],
   "source": [
    "# Pad sequences to a fixed length\n",
    "max_sequence_length = 100  # Adjust as needed\n",
    "X_train_padded = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_sequence_length, padding='post')\n",
    "X_test_padded = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_sequence_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDijFWMG1dAI"
   },
   "source": [
    "one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "DL3MP_AG1G8L"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the 'overall' column of your training data\n",
    "label_encoder.fit(train_data['overall'])\n",
    "\n",
    "# Transform the labels to integer-encoded labels\n",
    "y_train_encoded = label_encoder.transform(train_data['overall'])\n",
    "y_test_encoded = label_encoder.transform(test_data['overall'])\n",
    "\n",
    "# Convert encoded labels to one-hot encoded vectors\n",
    "num_classes = len(label_encoder.classes_)\n",
    "y_train = tf.keras.utils.to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test_encoded, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "y1uFWoMU1fxU"
   },
   "outputs": [],
   "source": [
    "# Define a function to create and train an RNN model for multiclass classification\n",
    "def train_rnn_multiclass(num_layers, layer_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=300, input_length=max_sequence_length))\n",
    "    for _ in range(num_layers):\n",
    "        model.add(tf.keras.layers.SimpleRNN(units=layer_size, return_sequences=True))\n",
    "    model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "    model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))  # Use softmax for multiclass\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train_padded, y_train, epochs=5, batch_size=16, validation_split=0.1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XOZVyeSE4rtn",
    "outputId": "18804994-384b-43b4-c6ac-ea49d4b669e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RNN with 1 layers and size 20 for multiclass\n",
      "Epoch 1/5\n",
      "192/192 [==============================] - 24s 103ms/step - loss: 1.3068 - accuracy: 0.5263 - val_loss: 1.1560 - val_accuracy: 0.5894\n",
      "Epoch 2/5\n",
      "192/192 [==============================] - 15s 77ms/step - loss: 1.0793 - accuracy: 0.5897 - val_loss: 1.0797 - val_accuracy: 0.6070\n",
      "Epoch 3/5\n",
      "192/192 [==============================] - 12s 64ms/step - loss: 0.8504 - accuracy: 0.6981 - val_loss: 1.0997 - val_accuracy: 0.5894\n",
      "Epoch 4/5\n",
      "192/192 [==============================] - 14s 71ms/step - loss: 0.6573 - accuracy: 0.7883 - val_loss: 1.1038 - val_accuracy: 0.5894\n",
      "Epoch 5/5\n",
      "192/192 [==============================] - 15s 78ms/step - loss: 0.4962 - accuracy: 0.8670 - val_loss: 1.1876 - val_accuracy: 0.5630\n",
      "31/31 [==============================] - 1s 12ms/step\n",
      "RNN Multiclass Accuracy: 0.5221\n",
      "Training RNN with 1 layers and size 50 for multiclass\n",
      "Epoch 1/5\n",
      "192/192 [==============================] - 18s 87ms/step - loss: 1.2601 - accuracy: 0.5397 - val_loss: 1.1872 - val_accuracy: 0.5894\n",
      "Epoch 2/5\n",
      "192/192 [==============================] - 16s 85ms/step - loss: 1.1541 - accuracy: 0.5599 - val_loss: 1.1406 - val_accuracy: 0.5894\n",
      "Epoch 3/5\n",
      "192/192 [==============================] - 17s 87ms/step - loss: 0.9648 - accuracy: 0.6527 - val_loss: 1.0752 - val_accuracy: 0.6041\n",
      "Epoch 4/5\n",
      "192/192 [==============================] - 21s 108ms/step - loss: 0.7397 - accuracy: 0.7455 - val_loss: 1.1000 - val_accuracy: 0.6100\n",
      "Epoch 5/5\n",
      "192/192 [==============================] - 18s 93ms/step - loss: 0.5626 - accuracy: 0.8272 - val_loss: 1.0942 - val_accuracy: 0.5806\n",
      "31/31 [==============================] - 1s 14ms/step\n",
      "RNN Multiclass Accuracy: 0.5848\n",
      "Training RNN with 1 layers and size 100 for multiclass\n",
      "Epoch 1/5\n",
      "192/192 [==============================] - 21s 102ms/step - loss: 1.2673 - accuracy: 0.5482 - val_loss: 1.1928 - val_accuracy: 0.5894\n",
      "Epoch 2/5\n",
      "192/192 [==============================] - 23s 122ms/step - loss: 1.2295 - accuracy: 0.5567 - val_loss: 1.1109 - val_accuracy: 0.5894\n",
      "Epoch 3/5\n",
      "192/192 [==============================] - 32s 164ms/step - loss: 1.1182 - accuracy: 0.5697 - val_loss: 1.0953 - val_accuracy: 0.6041\n",
      "Epoch 4/5\n",
      "192/192 [==============================] - 23s 118ms/step - loss: 0.8904 - accuracy: 0.6534 - val_loss: 0.9984 - val_accuracy: 0.6158\n",
      "Epoch 5/5\n",
      "192/192 [==============================] - 19s 98ms/step - loss: 0.6374 - accuracy: 0.7961 - val_loss: 1.0077 - val_accuracy: 0.6158\n",
      "31/31 [==============================] - 1s 20ms/step\n",
      "RNN Multiclass Accuracy: 0.5899\n",
      "Training RNN with 1 layers and size 200 for multiclass\n",
      "Epoch 1/5\n",
      "192/192 [==============================] - 27s 134ms/step - loss: 1.2848 - accuracy: 0.5459 - val_loss: 1.1968 - val_accuracy: 0.5894\n",
      "Epoch 2/5\n",
      "192/192 [==============================] - 28s 145ms/step - loss: 1.2614 - accuracy: 0.5577 - val_loss: 1.2192 - val_accuracy: 0.5894\n",
      "Epoch 3/5\n",
      "192/192 [==============================] - 25s 131ms/step - loss: 1.2640 - accuracy: 0.5577 - val_loss: 1.2140 - val_accuracy: 0.5894\n",
      "Epoch 4/5\n",
      "192/192 [==============================] - 30s 158ms/step - loss: 1.2609 - accuracy: 0.5577 - val_loss: 1.2140 - val_accuracy: 0.5894\n",
      "Epoch 5/5\n",
      "192/192 [==============================] - 31s 162ms/step - loss: 1.2542 - accuracy: 0.5577 - val_loss: 1.1919 - val_accuracy: 0.5894\n",
      "31/31 [==============================] - 1s 37ms/step\n",
      "RNN Multiclass Accuracy: 0.5910\n",
      "Training RNN with 2 layers and size 20 for multiclass\n",
      "Epoch 1/5\n",
      "192/192 [==============================] - 22s 102ms/step - loss: 1.2300 - accuracy: 0.5557 - val_loss: 1.0998 - val_accuracy: 0.5894\n",
      "Epoch 2/5\n",
      "192/192 [==============================] - 22s 117ms/step - loss: 1.0586 - accuracy: 0.5799 - val_loss: 1.0297 - val_accuracy: 0.6158\n",
      "Epoch 3/5\n",
      "192/192 [==============================] - 19s 100ms/step - loss: 0.8709 - accuracy: 0.6671 - val_loss: 1.0067 - val_accuracy: 0.6100\n",
      "Epoch 4/5\n",
      "192/192 [==============================] - 20s 103ms/step - loss: 0.6886 - accuracy: 0.7631 - val_loss: 1.0180 - val_accuracy: 0.5865\n",
      "Epoch 5/5\n",
      "192/192 [==============================] - 21s 108ms/step - loss: 0.5141 - accuracy: 0.8422 - val_loss: 1.1694 - val_accuracy: 0.5367\n",
      "31/31 [==============================] - 1s 18ms/step\n",
      "RNN Multiclass Accuracy: 0.4851\n",
      "Training RNN with 2 layers and size 50 for multiclass\n",
      "Epoch 1/5\n",
      "192/192 [==============================] - 25s 119ms/step - loss: 1.2302 - accuracy: 0.5570 - val_loss: 1.1109 - val_accuracy: 0.5894\n",
      "Epoch 2/5\n",
      "192/192 [==============================] - 20s 107ms/step - loss: 1.0392 - accuracy: 0.5926 - val_loss: 1.1310 - val_accuracy: 0.5953\n",
      "Epoch 3/5\n",
      "192/192 [==============================] - 22s 117ms/step - loss: 0.8478 - accuracy: 0.6906 - val_loss: 1.1195 - val_accuracy: 0.5543\n",
      "Epoch 4/5\n",
      "192/192 [==============================] - 21s 110ms/step - loss: 0.6356 - accuracy: 0.7880 - val_loss: 1.1067 - val_accuracy: 0.5748\n",
      "Epoch 5/5\n",
      "192/192 [==============================] - 25s 130ms/step - loss: 0.4380 - accuracy: 0.8798 - val_loss: 1.1808 - val_accuracy: 0.6012\n",
      "31/31 [==============================] - 1s 21ms/step\n",
      "RNN Multiclass Accuracy: 0.5642\n",
      "Training RNN with 2 layers and size 100 for multiclass\n",
      "Epoch 1/5\n",
      "192/192 [==============================] - 30s 133ms/step - loss: 1.2595 - accuracy: 0.5524 - val_loss: 1.1484 - val_accuracy: 0.5894\n",
      "Epoch 2/5\n",
      "192/192 [==============================] - 25s 129ms/step - loss: 1.1036 - accuracy: 0.5737 - val_loss: 1.1298 - val_accuracy: 0.5455\n",
      "Epoch 3/5\n",
      "192/192 [==============================] - 25s 128ms/step - loss: 0.8803 - accuracy: 0.6825 - val_loss: 1.1985 - val_accuracy: 0.5367\n",
      "Epoch 4/5\n",
      "192/192 [==============================] - 24s 125ms/step - loss: 0.6405 - accuracy: 0.7854 - val_loss: 1.2289 - val_accuracy: 0.5543\n",
      "Epoch 5/5\n",
      "192/192 [==============================] - 25s 130ms/step - loss: 0.3960 - accuracy: 0.8791 - val_loss: 1.3165 - val_accuracy: 0.5308\n",
      "31/31 [==============================] - 1s 28ms/step\n",
      "RNN Multiclass Accuracy: 0.5632\n",
      "Training RNN with 2 layers and size 200 for multiclass\n",
      "Epoch 1/5\n",
      "192/192 [==============================] - 38s 187ms/step - loss: 1.2527 - accuracy: 0.5531 - val_loss: 1.2512 - val_accuracy: 0.5894\n",
      "Epoch 2/5\n",
      "192/192 [==============================] - 31s 163ms/step - loss: 1.2677 - accuracy: 0.5583 - val_loss: 1.1998 - val_accuracy: 0.5894\n",
      "Epoch 3/5\n",
      "192/192 [==============================] - 32s 165ms/step - loss: 1.2493 - accuracy: 0.5577 - val_loss: 1.1836 - val_accuracy: 0.5894\n",
      "Epoch 4/5\n",
      "192/192 [==============================] - 33s 170ms/step - loss: 1.2160 - accuracy: 0.5521 - val_loss: 1.1979 - val_accuracy: 0.5894\n",
      "Epoch 5/5\n",
      "192/192 [==============================] - 34s 177ms/step - loss: 1.2450 - accuracy: 0.5554 - val_loss: 1.1948 - val_accuracy: 0.5894\n",
      "31/31 [==============================] - 2s 52ms/step\n",
      "RNN Multiclass Accuracy: 0.5910\n",
      "Training RNN with 3 layers and size 20 for multiclass\n",
      "Epoch 1/5\n",
      "192/192 [==============================] - 24s 111ms/step - loss: 1.2586 - accuracy: 0.5335 - val_loss: 1.1221 - val_accuracy: 0.5953\n",
      "Epoch 2/5\n",
      "192/192 [==============================] - 19s 100ms/step - loss: 1.0775 - accuracy: 0.5933 - val_loss: 1.0926 - val_accuracy: 0.5836\n",
      "Epoch 3/5\n",
      "192/192 [==============================] - 19s 101ms/step - loss: 0.8858 - accuracy: 0.6707 - val_loss: 1.1182 - val_accuracy: 0.5748\n",
      "Epoch 4/5\n",
      "192/192 [==============================] - 21s 110ms/step - loss: 0.6987 - accuracy: 0.7618 - val_loss: 1.2391 - val_accuracy: 0.4985\n",
      "Epoch 5/5\n",
      "192/192 [==============================] - 19s 101ms/step - loss: 0.5414 - accuracy: 0.8233 - val_loss: 1.3567 - val_accuracy: 0.5103\n",
      "31/31 [==============================] - 2s 36ms/step\n",
      "RNN Multiclass Accuracy: 0.4964\n",
      "Training RNN with 3 layers and size 50 for multiclass\n",
      "Epoch 1/5\n",
      "192/192 [==============================] - 28s 128ms/step - loss: 1.2526 - accuracy: 0.5495 - val_loss: 1.1480 - val_accuracy: 0.5894\n",
      "Epoch 2/5\n",
      "192/192 [==============================] - 25s 131ms/step - loss: 1.1354 - accuracy: 0.5678 - val_loss: 1.1075 - val_accuracy: 0.5924\n",
      "Epoch 3/5\n",
      "192/192 [==============================] - 25s 130ms/step - loss: 0.9542 - accuracy: 0.6527 - val_loss: 1.1208 - val_accuracy: 0.5806\n",
      "Epoch 4/5\n",
      "192/192 [==============================] - 22s 117ms/step - loss: 0.7220 - accuracy: 0.7553 - val_loss: 1.1666 - val_accuracy: 0.5367\n",
      "Epoch 5/5\n",
      "192/192 [==============================] - 23s 118ms/step - loss: 0.4729 - accuracy: 0.8651 - val_loss: 1.2700 - val_accuracy: 0.5249\n",
      "31/31 [==============================] - 1s 25ms/step\n",
      "RNN Multiclass Accuracy: 0.5242\n",
      "Training RNN with 3 layers and size 100 for multiclass\n",
      "Epoch 1/5\n",
      "192/192 [==============================] - 32s 148ms/step - loss: 1.2516 - accuracy: 0.5528 - val_loss: 1.1426 - val_accuracy: 0.5894\n",
      "Epoch 2/5\n",
      "192/192 [==============================] - 28s 146ms/step - loss: 1.1667 - accuracy: 0.5550 - val_loss: 1.0780 - val_accuracy: 0.5806\n",
      "Epoch 3/5\n",
      "192/192 [==============================] - 28s 144ms/step - loss: 1.0389 - accuracy: 0.5841 - val_loss: 1.1077 - val_accuracy: 0.5777\n",
      "Epoch 4/5\n",
      "192/192 [==============================] - 30s 156ms/step - loss: 0.9234 - accuracy: 0.6237 - val_loss: 1.1827 - val_accuracy: 0.6012\n",
      "Epoch 5/5\n",
      "192/192 [==============================] - 29s 148ms/step - loss: 0.8392 - accuracy: 0.6645 - val_loss: 1.1536 - val_accuracy: 0.5601\n",
      "31/31 [==============================] - 1s 33ms/step\n",
      "RNN Multiclass Accuracy: 0.5725\n",
      "Training RNN with 3 layers and size 200 for multiclass\n",
      "Epoch 1/5\n",
      "192/192 [==============================] - 46s 222ms/step - loss: 1.2776 - accuracy: 0.5452 - val_loss: 1.1892 - val_accuracy: 0.5894\n",
      "Epoch 2/5\n",
      "192/192 [==============================] - 42s 221ms/step - loss: 1.2065 - accuracy: 0.5554 - val_loss: 1.1806 - val_accuracy: 0.5836\n",
      "Epoch 3/5\n",
      "192/192 [==============================] - 42s 220ms/step - loss: 1.1303 - accuracy: 0.5730 - val_loss: 1.2799 - val_accuracy: 0.4985\n",
      "Epoch 4/5\n",
      "192/192 [==============================] - 45s 233ms/step - loss: 1.0985 - accuracy: 0.6044 - val_loss: 1.1874 - val_accuracy: 0.5894\n",
      "Epoch 5/5\n",
      "192/192 [==============================] - 44s 227ms/step - loss: 1.1939 - accuracy: 0.5580 - val_loss: 1.2036 - val_accuracy: 0.5718\n",
      "31/31 [==============================] - 4s 111ms/step\n",
      "RNN Multiclass Accuracy: 0.5498\n",
      "Training RNN with 4 layers and size 20 for multiclass\n",
      "Epoch 1/5\n",
      "192/192 [==============================] - 30s 135ms/step - loss: 1.3335 - accuracy: 0.4998 - val_loss: 1.1605 - val_accuracy: 0.5924\n",
      "Epoch 2/5\n",
      "192/192 [==============================] - 36s 189ms/step - loss: 1.1477 - accuracy: 0.5720 - val_loss: 1.1647 - val_accuracy: 0.5894\n",
      "Epoch 3/5\n",
      "192/192 [==============================] - 26s 136ms/step - loss: 1.0131 - accuracy: 0.6246 - val_loss: 1.1933 - val_accuracy: 0.5455\n",
      "Epoch 4/5\n",
      "192/192 [==============================] - 24s 125ms/step - loss: 0.8276 - accuracy: 0.7141 - val_loss: 1.2659 - val_accuracy: 0.5132\n",
      "Epoch 5/5\n",
      "192/192 [==============================] - 22s 116ms/step - loss: 0.6147 - accuracy: 0.8040 - val_loss: 1.4151 - val_accuracy: 0.5073\n",
      "31/31 [==============================] - 1s 23ms/step\n",
      "RNN Multiclass Accuracy: 0.5457\n",
      "Training RNN with 4 layers and size 50 for multiclass\n",
      "Epoch 1/5\n",
      "192/192 [==============================] - 32s 135ms/step - loss: 1.2851 - accuracy: 0.5420 - val_loss: 1.1722 - val_accuracy: 0.5894\n",
      "Epoch 2/5\n",
      "192/192 [==============================] - 34s 178ms/step - loss: 1.1223 - accuracy: 0.5776 - val_loss: 1.1652 - val_accuracy: 0.5894\n",
      "Epoch 3/5\n",
      "192/192 [==============================] - 31s 163ms/step - loss: 0.9653 - accuracy: 0.6462 - val_loss: 1.1944 - val_accuracy: 0.5367\n",
      "Epoch 4/5\n",
      "192/192 [==============================] - 26s 135ms/step - loss: 0.7968 - accuracy: 0.7132 - val_loss: 1.2302 - val_accuracy: 0.5484\n",
      "Epoch 5/5\n",
      "192/192 [==============================] - 26s 133ms/step - loss: 0.6424 - accuracy: 0.7674 - val_loss: 1.3826 - val_accuracy: 0.4956\n",
      "31/31 [==============================] - 1s 30ms/step\n",
      "RNN Multiclass Accuracy: 0.4943\n",
      "Training RNN with 4 layers and size 100 for multiclass\n",
      "Epoch 1/5\n",
      "192/192 [==============================] - 38s 174ms/step - loss: 1.2537 - accuracy: 0.5485 - val_loss: 1.1936 - val_accuracy: 0.5894\n",
      "Epoch 2/5\n",
      "192/192 [==============================] - 34s 178ms/step - loss: 1.2082 - accuracy: 0.5547 - val_loss: 1.1505 - val_accuracy: 0.5894\n",
      "Epoch 3/5\n",
      "192/192 [==============================] - 34s 176ms/step - loss: 1.1788 - accuracy: 0.5511 - val_loss: 1.1784 - val_accuracy: 0.5777\n",
      "Epoch 4/5\n",
      "192/192 [==============================] - 32s 167ms/step - loss: 1.1462 - accuracy: 0.5704 - val_loss: 1.2187 - val_accuracy: 0.5777\n",
      "Epoch 5/5\n",
      "192/192 [==============================] - 36s 188ms/step - loss: 1.1443 - accuracy: 0.5743 - val_loss: 1.1573 - val_accuracy: 0.5748\n",
      "31/31 [==============================] - 2s 40ms/step\n",
      "RNN Multiclass Accuracy: 0.5663\n",
      "Training RNN with 4 layers and size 200 for multiclass\n",
      "Epoch 1/5\n",
      "192/192 [==============================] - 58s 281ms/step - loss: 1.2842 - accuracy: 0.5511 - val_loss: 1.2176 - val_accuracy: 0.5894\n",
      "Epoch 2/5\n",
      "192/192 [==============================] - 54s 283ms/step - loss: 1.2603 - accuracy: 0.5577 - val_loss: 1.1910 - val_accuracy: 0.5894\n",
      "Epoch 3/5\n",
      "192/192 [==============================] - 53s 278ms/step - loss: 1.2547 - accuracy: 0.5577 - val_loss: 1.2197 - val_accuracy: 0.5894\n",
      "Epoch 4/5\n",
      "192/192 [==============================] - 54s 282ms/step - loss: 1.2585 - accuracy: 0.5521 - val_loss: 1.2020 - val_accuracy: 0.5718\n",
      "Epoch 5/5\n",
      "192/192 [==============================] - 58s 301ms/step - loss: 1.2455 - accuracy: 0.5541 - val_loss: 1.1918 - val_accuracy: 0.5894\n",
      "31/31 [==============================] - 3s 83ms/step\n",
      "RNN Multiclass Accuracy: 0.5910\n"
     ]
    }
   ],
   "source": [
    "# Train LSTMs with different layer configurations for multiclass classification\n",
    "layer_sizes = [20, 50, 100, 200]\n",
    "results_rnn_multiclass = {}\n",
    "\n",
    "for num_layers in [1, 2, 3, 4]:\n",
    "    for layer_size in layer_sizes:\n",
    "        print(f\"Training RNN with {num_layers} layers and size {layer_size} for multiclass\")\n",
    "        model = train_rnn_multiclass(num_layers, layer_size)\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        y_pred = model.predict(X_test_padded)\n",
    "        y_pred_class = np.argmax(y_pred, axis=1)  # Convert probabilities to predicted class\n",
    "        accuracy = accuracy_score(np.argmax(y_test, axis=1), y_pred_class)\n",
    "        results_rnn_multiclass[(num_layers, layer_size)] = accuracy\n",
    "        print(f\"RNN Multiclass Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U9ne7oFU5Fjv",
    "outputId": "d75828a7-64dd-4d1f-c5e9-8a674ee1287d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for Multiclass RNN:\n",
      "RNN with 1 layers and size 20: Accuracy - 0.5221\n",
      "RNN with 1 layers and size 50: Accuracy - 0.5848\n",
      "RNN with 1 layers and size 100: Accuracy - 0.5899\n",
      "RNN with 1 layers and size 200: Accuracy - 0.5910\n",
      "RNN with 2 layers and size 20: Accuracy - 0.4851\n",
      "RNN with 2 layers and size 50: Accuracy - 0.5642\n",
      "RNN with 2 layers and size 100: Accuracy - 0.5632\n",
      "RNN with 2 layers and size 200: Accuracy - 0.5910\n",
      "RNN with 3 layers and size 20: Accuracy - 0.4964\n",
      "RNN with 3 layers and size 50: Accuracy - 0.5242\n",
      "RNN with 3 layers and size 100: Accuracy - 0.5725\n",
      "RNN with 3 layers and size 200: Accuracy - 0.5498\n",
      "RNN with 4 layers and size 20: Accuracy - 0.5457\n",
      "RNN with 4 layers and size 50: Accuracy - 0.4943\n",
      "RNN with 4 layers and size 100: Accuracy - 0.5663\n",
      "RNN with 4 layers and size 200: Accuracy - 0.5910\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(\"\\nResults for Multiclass RNN:\")\n",
    "for layer_config, accuracy in results_rnn_multiclass.items():\n",
    "    num_layers, layer_size = layer_config\n",
    "    print(f\"RNN with {num_layers} layers and size {layer_size}: Accuracy - {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_bvKgfPfe6o"
   },
   "source": [
    "**The best performing model was RNN with 1 layers and size 200: Accuracy - 0.5910, RNN with 2 layers and size 200: Accuracy - 0.5910,\n",
    "RNN with 4 layers and size 200: Accuracy - 0.5910**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkM6U2EO1qAj"
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewd7CsFJrggv"
   },
   "source": [
    "4.b. **Long Short-Term Memory (LSTM):**\n",
    "   - **Description:** LSTM is an advanced RNN variant designed to alleviate vanishing gradient problem, making it better at learning long-range dependencies.\n",
    "   - **Advantages:** Handles long sequences, captures context well, mitigates vanishing gradient.\n",
    "   - **Disadvantages:** Complex architecture, training can be slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "jRN4mByW1rLX"
   },
   "outputs": [],
   "source": [
    "# Define a function to create and train an LSTM model for multiclass classification\n",
    "def train_lstm_multiclass(num_layers, layer_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=300, input_length=max_sequence_length))\n",
    "    for _ in range(num_layers):\n",
    "        model.add(tf.keras.layers.LSTM(units=layer_size, return_sequences=True))\n",
    "    model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "    model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))  # Use softmax for multiclass\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train_padded, y_train, epochs=5, batch_size=32, validation_split=0.1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x7-Ty7xQ4RPy",
    "outputId": "a29058fb-61a1-4ba6-dfac-f4cddaf504ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM with 1 layers and size 20 for multiclass\n",
      "Epoch 1/5\n",
      "96/96 [==============================] - 18s 159ms/step - loss: 1.3453 - accuracy: 0.5335 - val_loss: 1.1845 - val_accuracy: 0.5894\n",
      "Epoch 2/5\n",
      "96/96 [==============================] - 13s 139ms/step - loss: 1.1547 - accuracy: 0.5577 - val_loss: 1.0482 - val_accuracy: 0.5894\n",
      "Epoch 3/5\n",
      "96/96 [==============================] - 12s 129ms/step - loss: 0.9618 - accuracy: 0.6073 - val_loss: 0.9925 - val_accuracy: 0.6246\n",
      "Epoch 4/5\n",
      "96/96 [==============================] - 10s 107ms/step - loss: 0.7636 - accuracy: 0.7318 - val_loss: 1.0362 - val_accuracy: 0.6041\n",
      "Epoch 5/5\n",
      "96/96 [==============================] - 12s 125ms/step - loss: 0.6001 - accuracy: 0.7978 - val_loss: 1.1250 - val_accuracy: 0.5982\n",
      "31/31 [==============================] - 1s 15ms/step\n",
      "LSTM Multiclass Accuracy: 0.5982\n",
      "Training LSTM with 1 layers and size 100 for multiclass\n",
      "Epoch 1/5\n",
      "96/96 [==============================] - 27s 260ms/step - loss: 1.2612 - accuracy: 0.5524 - val_loss: 1.1135 - val_accuracy: 0.5894\n",
      "Epoch 2/5\n",
      "96/96 [==============================] - 23s 237ms/step - loss: 1.0619 - accuracy: 0.5756 - val_loss: 1.0220 - val_accuracy: 0.6129\n",
      "Epoch 3/5\n",
      "96/96 [==============================] - 25s 258ms/step - loss: 0.8362 - accuracy: 0.6746 - val_loss: 0.9999 - val_accuracy: 0.6188\n",
      "Epoch 4/5\n",
      "96/96 [==============================] - 27s 280ms/step - loss: 0.6307 - accuracy: 0.7671 - val_loss: 1.0818 - val_accuracy: 0.6217\n",
      "Epoch 5/5\n",
      "96/96 [==============================] - 22s 233ms/step - loss: 0.4688 - accuracy: 0.8314 - val_loss: 1.2440 - val_accuracy: 0.5660\n",
      "31/31 [==============================] - 3s 89ms/step\n",
      "LSTM Multiclass Accuracy: 0.5324\n",
      "Training LSTM with 2 layers and size 20 for multiclass\n",
      "Epoch 1/5\n",
      "96/96 [==============================] - 21s 170ms/step - loss: 1.3142 - accuracy: 0.5475 - val_loss: 1.1686 - val_accuracy: 0.5894\n",
      "Epoch 2/5\n",
      "96/96 [==============================] - 17s 174ms/step - loss: 1.1529 - accuracy: 0.5573 - val_loss: 1.0805 - val_accuracy: 0.5924\n",
      "Epoch 3/5\n",
      "96/96 [==============================] - 20s 209ms/step - loss: 0.9949 - accuracy: 0.6142 - val_loss: 1.0458 - val_accuracy: 0.6012\n",
      "Epoch 4/5\n",
      "96/96 [==============================] - 18s 184ms/step - loss: 0.8417 - accuracy: 0.6766 - val_loss: 1.0363 - val_accuracy: 0.5836\n",
      "Epoch 5/5\n",
      "96/96 [==============================] - 18s 184ms/step - loss: 0.7501 - accuracy: 0.7207 - val_loss: 1.0587 - val_accuracy: 0.6041\n",
      "31/31 [==============================] - 2s 23ms/step\n",
      "LSTM Multiclass Accuracy: 0.5807\n",
      "Training LSTM with 2 layers and size 100 for multiclass\n",
      "Epoch 1/5\n",
      "96/96 [==============================] - 39s 350ms/step - loss: 1.2603 - accuracy: 0.5508 - val_loss: 1.1204 - val_accuracy: 0.5894\n",
      "Epoch 2/5\n",
      "96/96 [==============================] - 36s 380ms/step - loss: 1.1150 - accuracy: 0.5681 - val_loss: 1.0394 - val_accuracy: 0.6070\n",
      "Epoch 3/5\n",
      "96/96 [==============================] - 39s 409ms/step - loss: 0.9167 - accuracy: 0.6318 - val_loss: 1.0737 - val_accuracy: 0.5836\n",
      "Epoch 4/5\n",
      "96/96 [==============================] - 33s 345ms/step - loss: 0.7337 - accuracy: 0.7106 - val_loss: 1.1439 - val_accuracy: 0.5572\n",
      "Epoch 5/5\n",
      "96/96 [==============================] - 35s 359ms/step - loss: 0.5563 - accuracy: 0.7847 - val_loss: 1.3369 - val_accuracy: 0.5425\n",
      "31/31 [==============================] - 3s 75ms/step\n",
      "LSTM Multiclass Accuracy: 0.5324\n",
      "Training LSTM with 3 layers and size 20 for multiclass\n",
      "Epoch 1/5\n",
      "96/96 [==============================] - 27s 198ms/step - loss: 1.2926 - accuracy: 0.5577 - val_loss: 1.1546 - val_accuracy: 0.5894\n",
      "Epoch 2/5\n",
      "96/96 [==============================] - 18s 186ms/step - loss: 1.1308 - accuracy: 0.5577 - val_loss: 1.0572 - val_accuracy: 0.5924\n",
      "Epoch 3/5\n",
      "96/96 [==============================] - 19s 197ms/step - loss: 0.9906 - accuracy: 0.5959 - val_loss: 1.0353 - val_accuracy: 0.5689\n",
      "Epoch 4/5\n",
      "96/96 [==============================] - 18s 188ms/step - loss: 0.8528 - accuracy: 0.6468 - val_loss: 1.1006 - val_accuracy: 0.5689\n",
      "Epoch 5/5\n",
      "96/96 [==============================] - 18s 184ms/step - loss: 0.7398 - accuracy: 0.7115 - val_loss: 1.1550 - val_accuracy: 0.5044\n",
      "31/31 [==============================] - 2s 33ms/step\n",
      "LSTM Multiclass Accuracy: 0.4882\n",
      "Training LSTM with 3 layers and size 100 for multiclass\n",
      "Epoch 1/5\n",
      "96/96 [==============================] - 54s 485ms/step - loss: 1.2698 - accuracy: 0.5557 - val_loss: 1.1941 - val_accuracy: 0.5894\n",
      "Epoch 2/5\n",
      "96/96 [==============================] - 46s 476ms/step - loss: 1.1869 - accuracy: 0.5577 - val_loss: 1.0697 - val_accuracy: 0.5894\n",
      "Epoch 3/5\n",
      "96/96 [==============================] - 43s 448ms/step - loss: 1.0127 - accuracy: 0.5874 - val_loss: 1.0713 - val_accuracy: 0.5865\n",
      "Epoch 4/5\n",
      "96/96 [==============================] - 43s 449ms/step - loss: 0.8630 - accuracy: 0.6475 - val_loss: 1.1850 - val_accuracy: 0.5396\n",
      "Epoch 5/5\n",
      "96/96 [==============================] - 43s 449ms/step - loss: 0.7241 - accuracy: 0.7158 - val_loss: 1.2560 - val_accuracy: 0.5044\n",
      "31/31 [==============================] - 6s 105ms/step\n",
      "LSTM Multiclass Accuracy: 0.4872\n",
      "Training LSTM with 4 layers and size 20 for multiclass\n",
      "Epoch 1/5\n",
      "96/96 [==============================] - 34s 258ms/step - loss: 1.2912 - accuracy: 0.5515 - val_loss: 1.1735 - val_accuracy: 0.5894\n",
      "Epoch 2/5\n",
      "96/96 [==============================] - 21s 224ms/step - loss: 1.1357 - accuracy: 0.5635 - val_loss: 1.0902 - val_accuracy: 0.5924\n",
      "Epoch 3/5\n",
      "96/96 [==============================] - 23s 241ms/step - loss: 0.9744 - accuracy: 0.6132 - val_loss: 1.0288 - val_accuracy: 0.5806\n",
      "Epoch 4/5\n",
      "96/96 [==============================] - 22s 224ms/step - loss: 0.8166 - accuracy: 0.6952 - val_loss: 1.0933 - val_accuracy: 0.6012\n",
      "Epoch 5/5\n",
      "96/96 [==============================] - 23s 240ms/step - loss: 0.7144 - accuracy: 0.7386 - val_loss: 1.0872 - val_accuracy: 0.6100\n",
      "31/31 [==============================] - 3s 41ms/step\n",
      "LSTM Multiclass Accuracy: 0.5581\n",
      "Training LSTM with 4 layers and size 100 for multiclass\n",
      "Epoch 1/5\n",
      "96/96 [==============================] - 67s 605ms/step - loss: 1.2665 - accuracy: 0.5511 - val_loss: 1.1701 - val_accuracy: 0.5894\n",
      "Epoch 2/5\n",
      "96/96 [==============================] - 54s 560ms/step - loss: 1.1717 - accuracy: 0.5577 - val_loss: 1.0837 - val_accuracy: 0.5894\n",
      "Epoch 3/5\n",
      "96/96 [==============================] - 53s 549ms/step - loss: 1.0662 - accuracy: 0.5665 - val_loss: 1.0918 - val_accuracy: 0.5777\n",
      "Epoch 4/5\n",
      "96/96 [==============================] - 57s 593ms/step - loss: 0.9854 - accuracy: 0.5897 - val_loss: 1.1471 - val_accuracy: 0.5249\n",
      "Epoch 5/5\n",
      "96/96 [==============================] - 56s 581ms/step - loss: 0.8909 - accuracy: 0.6325 - val_loss: 1.1806 - val_accuracy: 0.4839\n",
      "31/31 [==============================] - 7s 179ms/step\n",
      "LSTM Multiclass Accuracy: 0.4522\n"
     ]
    }
   ],
   "source": [
    "# Train LSTMs with different layer configurations for multiclass classification\n",
    "layer_sizes = [20, 100]\n",
    "results_lstm_multiclass = {}\n",
    "\n",
    "for num_layers in [1, 2, 3, 4]:\n",
    "    for layer_size in layer_sizes:\n",
    "        print(f\"Training LSTM with {num_layers} layers and size {layer_size} for multiclass\")\n",
    "        model = train_lstm_multiclass(num_layers, layer_size)\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        y_pred = model.predict(X_test_padded)\n",
    "        y_pred_class = np.argmax(y_pred, axis=1)  # Convert probabilities to predicted class\n",
    "        accuracy = accuracy_score(np.argmax(y_test, axis=1), y_pred_class)\n",
    "        results_lstm_multiclass[(num_layers, layer_size)] = accuracy\n",
    "        print(f\"LSTM Multiclass Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5lpVPGkJ4h2F",
    "outputId": "ef6b00c8-6dc6-4319-f36b-e0cccd9c68d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for Multiclass LSTM:\n",
      "LSTM with 1 layers and size 20: Accuracy - 0.5982\n",
      "LSTM with 1 layers and size 100: Accuracy - 0.5324\n",
      "LSTM with 2 layers and size 20: Accuracy - 0.5807\n",
      "LSTM with 2 layers and size 100: Accuracy - 0.5324\n",
      "LSTM with 3 layers and size 20: Accuracy - 0.4882\n",
      "LSTM with 3 layers and size 100: Accuracy - 0.4872\n",
      "LSTM with 4 layers and size 20: Accuracy - 0.5581\n",
      "LSTM with 4 layers and size 100: Accuracy - 0.4522\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(\"\\nResults for Multiclass LSTM:\")\n",
    "for layer_config, accuracy in results_lstm_multiclass.items():\n",
    "    num_layers, layer_size = layer_config\n",
    "    print(f\"LSTM with {num_layers} layers and size {layer_size}: Accuracy - {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWjNvy0kgaU_"
   },
   "source": [
    "**The best performing model was LSTM with 1 layers and size 20: Accuracy - 0.5982**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ry4oiDu15XW"
   },
   "source": [
    "## BiLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfOfsUpAt_FK"
   },
   "source": [
    "4.c. **Bidirectional LSTM (BiLSTM):**\n",
    "   - **Description:** Extends LSTM by processing sequences in both forward and backward directions, capturing context from both past and future.\n",
    "   - **Advantages:** Enhanced context capture, useful for tasks needing full sequence understanding.\n",
    "   - **Disadvantages:** Doubles computational complexity, might lead to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "a-fwfQg216m8"
   },
   "outputs": [],
   "source": [
    "# Define a function to create and train a Bidirectional LSTM model for multiclass classification\n",
    "def train_bilstm_multiclass(num_layers, layer_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=300, input_length=max_sequence_length))\n",
    "    for _ in range(num_layers):\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=layer_size, return_sequences=True)))\n",
    "    model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "    model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))  # Use softmax for multiclass\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train_padded, y_train, epochs=5, batch_size=32, validation_split=0.1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PbrBKW0Z5ssS",
    "outputId": "c28a0728-84fd-4303-d888-d657394c5ae9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BiLSTM with 1 layers and size 20 for multiclass\n",
      "Epoch 1/5\n",
      "96/96 [==============================] - 22s 171ms/step - loss: 1.2731 - accuracy: 0.5577 - val_loss: 1.1584 - val_accuracy: 0.5894\n",
      "Epoch 2/5\n",
      "96/96 [==============================] - 15s 158ms/step - loss: 1.1244 - accuracy: 0.5626 - val_loss: 1.0090 - val_accuracy: 0.6041\n",
      "Epoch 3/5\n",
      "96/96 [==============================] - 15s 159ms/step - loss: 0.8658 - accuracy: 0.6671 - val_loss: 0.9974 - val_accuracy: 0.6188\n",
      "Epoch 4/5\n",
      "96/96 [==============================] - 15s 158ms/step - loss: 0.6230 - accuracy: 0.7752 - val_loss: 1.0428 - val_accuracy: 0.6158\n",
      "Epoch 5/5\n",
      "96/96 [==============================] - 15s 159ms/step - loss: 0.4445 - accuracy: 0.8605 - val_loss: 1.1642 - val_accuracy: 0.5924\n",
      "31/31 [==============================] - 2s 45ms/step\n",
      "BiLSTM Multiclass Accuracy: 0.5550\n",
      "Training BiLSTM with 1 layers and size 100 for multiclass\n",
      "Epoch 1/5\n",
      "96/96 [==============================] - 52s 494ms/step - loss: 1.2643 - accuracy: 0.5508 - val_loss: 1.1149 - val_accuracy: 0.5894\n",
      "Epoch 2/5\n",
      "96/96 [==============================] - 44s 459ms/step - loss: 1.0542 - accuracy: 0.5854 - val_loss: 0.9593 - val_accuracy: 0.6129\n",
      "Epoch 3/5\n",
      "96/96 [==============================] - 44s 460ms/step - loss: 0.7886 - accuracy: 0.6988 - val_loss: 1.0263 - val_accuracy: 0.5894\n",
      "Epoch 4/5\n",
      "96/96 [==============================] - 44s 463ms/step - loss: 0.5486 - accuracy: 0.7968 - val_loss: 1.1622 - val_accuracy: 0.5865\n",
      "Epoch 5/5\n",
      "96/96 [==============================] - 45s 468ms/step - loss: 0.3659 - accuracy: 0.8677 - val_loss: 1.3717 - val_accuracy: 0.5630\n",
      "31/31 [==============================] - 5s 134ms/step\n",
      "BiLSTM Multiclass Accuracy: 0.5211\n",
      "Training BiLSTM with 2 layers and size 20 for multiclass\n",
      "Epoch 1/5\n",
      "96/96 [==============================] - 33s 249ms/step - loss: 1.2726 - accuracy: 0.5511 - val_loss: 1.1244 - val_accuracy: 0.5894\n",
      "Epoch 2/5\n",
      "96/96 [==============================] - 23s 243ms/step - loss: 1.0845 - accuracy: 0.5665 - val_loss: 1.0215 - val_accuracy: 0.6129\n",
      "Epoch 3/5\n",
      "96/96 [==============================] - 22s 224ms/step - loss: 0.8957 - accuracy: 0.6521 - val_loss: 1.0501 - val_accuracy: 0.5982\n",
      "Epoch 4/5\n",
      "96/96 [==============================] - 24s 247ms/step - loss: 0.6832 - accuracy: 0.7501 - val_loss: 1.1364 - val_accuracy: 0.5718\n",
      "Epoch 5/5\n",
      "96/96 [==============================] - 21s 223ms/step - loss: 0.5372 - accuracy: 0.8017 - val_loss: 1.2182 - val_accuracy: 0.6070\n",
      "31/31 [==============================] - 4s 57ms/step\n",
      "BiLSTM Multiclass Accuracy: 0.5776\n",
      "Training BiLSTM with 2 layers and size 100 for multiclass\n",
      "Epoch 1/5\n",
      "96/96 [==============================] - 88s 815ms/step - loss: 1.2491 - accuracy: 0.5573 - val_loss: 1.0997 - val_accuracy: 0.5894\n",
      "Epoch 2/5\n",
      "96/96 [==============================] - 74s 772ms/step - loss: 1.0508 - accuracy: 0.5910 - val_loss: 1.0019 - val_accuracy: 0.6246\n",
      "Epoch 3/5\n",
      "96/96 [==============================] - 74s 773ms/step - loss: 0.8196 - accuracy: 0.6776 - val_loss: 1.0262 - val_accuracy: 0.6158\n",
      "Epoch 4/5\n",
      "96/96 [==============================] - 78s 816ms/step - loss: 0.5904 - accuracy: 0.7779 - val_loss: 1.0729 - val_accuracy: 0.6041\n",
      "Epoch 5/5\n",
      "96/96 [==============================] - 75s 783ms/step - loss: 0.4009 - accuracy: 0.8527 - val_loss: 1.3598 - val_accuracy: 0.6012\n",
      "31/31 [==============================] - 9s 227ms/step\n",
      "BiLSTM Multiclass Accuracy: 0.5766\n",
      "Training BiLSTM with 3 layers and size 20 for multiclass\n",
      "Epoch 1/5\n",
      "96/96 [==============================] - 47s 349ms/step - loss: 1.2638 - accuracy: 0.5537 - val_loss: 1.1821 - val_accuracy: 0.5894\n",
      "Epoch 2/5\n",
      "96/96 [==============================] - 32s 333ms/step - loss: 1.1662 - accuracy: 0.5648 - val_loss: 1.0550 - val_accuracy: 0.5894\n",
      "Epoch 3/5\n",
      "96/96 [==============================] - 30s 314ms/step - loss: 0.9178 - accuracy: 0.6498 - val_loss: 1.0686 - val_accuracy: 0.5660\n",
      "Epoch 4/5\n",
      "96/96 [==============================] - 33s 345ms/step - loss: 0.7245 - accuracy: 0.7295 - val_loss: 1.1753 - val_accuracy: 0.5660\n",
      "Epoch 5/5\n",
      "96/96 [==============================] - 31s 325ms/step - loss: 0.5876 - accuracy: 0.7801 - val_loss: 1.3192 - val_accuracy: 0.5630\n",
      "31/31 [==============================] - 4s 56ms/step\n",
      "BiLSTM Multiclass Accuracy: 0.5190\n",
      "Training BiLSTM with 3 layers and size 100 for multiclass\n",
      "Epoch 1/5\n",
      "96/96 [==============================] - 145s 1s/step - loss: 1.2537 - accuracy: 0.5541 - val_loss: 1.1262 - val_accuracy: 0.5894\n",
      "Epoch 2/5\n",
      "96/96 [==============================] - 107s 1s/step - loss: 1.0582 - accuracy: 0.5831 - val_loss: 1.0126 - val_accuracy: 0.5806\n",
      "Epoch 3/5\n",
      "96/96 [==============================] - 127s 1s/step - loss: 0.8033 - accuracy: 0.6916 - val_loss: 1.0547 - val_accuracy: 0.5865\n",
      "Epoch 4/5\n",
      "96/96 [==============================] - 135s 1s/step - loss: 0.5736 - accuracy: 0.7863 - val_loss: 1.3094 - val_accuracy: 0.5748\n",
      "Epoch 5/5\n",
      "96/96 [==============================] - 132s 1s/step - loss: 0.4344 - accuracy: 0.8494 - val_loss: 1.3558 - val_accuracy: 0.5748\n",
      "31/31 [==============================] - 13s 309ms/step\n",
      "BiLSTM Multiclass Accuracy: 0.5416\n",
      "Training BiLSTM with 4 layers and size 20 for multiclass\n",
      "Epoch 1/5\n",
      "96/96 [==============================] - 81s 566ms/step - loss: 1.2715 - accuracy: 0.5573 - val_loss: 1.1890 - val_accuracy: 0.5894\n",
      "Epoch 2/5\n",
      "96/96 [==============================] - 48s 505ms/step - loss: 1.1330 - accuracy: 0.5678 - val_loss: 1.0046 - val_accuracy: 0.6070\n",
      "Epoch 3/5\n",
      "96/96 [==============================] - 48s 505ms/step - loss: 0.9084 - accuracy: 0.6387 - val_loss: 1.0696 - val_accuracy: 0.5806\n",
      "Epoch 4/5\n",
      "96/96 [==============================] - 53s 553ms/step - loss: 0.7362 - accuracy: 0.7099 - val_loss: 1.0957 - val_accuracy: 0.5953\n",
      "Epoch 5/5\n",
      "96/96 [==============================] - 49s 511ms/step - loss: 0.6325 - accuracy: 0.7586 - val_loss: 1.2276 - val_accuracy: 0.5484\n",
      "31/31 [==============================] - 8s 124ms/step\n",
      "BiLSTM Multiclass Accuracy: 0.5200\n",
      "Training BiLSTM with 4 layers and size 100 for multiclass\n",
      "Epoch 1/5\n",
      "96/96 [==============================] - 214s 2s/step - loss: 1.2664 - accuracy: 0.5567 - val_loss: 1.1982 - val_accuracy: 0.5894\n",
      "Epoch 2/5\n",
      "96/96 [==============================] - 174s 2s/step - loss: 1.1913 - accuracy: 0.5482 - val_loss: 1.0604 - val_accuracy: 0.5894\n",
      "Epoch 3/5\n",
      "96/96 [==============================] - 178s 2s/step - loss: 0.9993 - accuracy: 0.6027 - val_loss: 1.0312 - val_accuracy: 0.5953\n",
      "Epoch 4/5\n",
      "96/96 [==============================] - 170s 2s/step - loss: 0.8196 - accuracy: 0.6700 - val_loss: 1.1090 - val_accuracy: 0.5836\n",
      "Epoch 5/5\n",
      "96/96 [==============================] - 134s 1s/step - loss: 0.7033 - accuracy: 0.7282 - val_loss: 1.1862 - val_accuracy: 0.5777\n",
      "31/31 [==============================] - 14s 322ms/step\n",
      "BiLSTM Multiclass Accuracy: 0.5498\n"
     ]
    }
   ],
   "source": [
    "# Train LSTMs with different layer configurations for multiclass classification\n",
    "layer_sizes = [20, 100]\n",
    "results_bilstm_multiclass = {}\n",
    "\n",
    "for num_layers in [1, 2, 3, 4]:\n",
    "    for layer_size in layer_sizes:\n",
    "        print(f\"Training BiLSTM with {num_layers} layers and size {layer_size} for multiclass\")\n",
    "        model = train_bilstm_multiclass(num_layers, layer_size)\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        y_pred = model.predict(X_test_padded)\n",
    "        y_pred_class = np.argmax(y_pred, axis=1)  # Convert probabilities to predicted class\n",
    "        accuracy = accuracy_score(np.argmax(y_test, axis=1), y_pred_class)\n",
    "        results_bilstm_multiclass[(num_layers, layer_size)] = accuracy\n",
    "        print(f\"BiLSTM Multiclass Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iDmB_VrH5wj5",
    "outputId": "baab910c-d3a4-4e5d-8b72-9af3a6111cac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for Multiclass BiLSTM:\n",
      "BiLSTM with 1 layers and size 20: Accuracy - 0.5550\n",
      "BiLSTM with 1 layers and size 100: Accuracy - 0.5211\n",
      "BiLSTM with 2 layers and size 20: Accuracy - 0.5776\n",
      "BiLSTM with 2 layers and size 100: Accuracy - 0.5766\n",
      "BiLSTM with 3 layers and size 20: Accuracy - 0.5190\n",
      "BiLSTM with 3 layers and size 100: Accuracy - 0.5416\n",
      "BiLSTM with 4 layers and size 20: Accuracy - 0.5200\n",
      "BiLSTM with 4 layers and size 100: Accuracy - 0.5498\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(\"\\nResults for Multiclass BiLSTM:\")\n",
    "for layer_config, accuracy in results_bilstm_multiclass.items():\n",
    "    num_layers, layer_size = layer_config\n",
    "    print(f\"BiLSTM with {num_layers} layers and size {layer_size}: Accuracy - {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rhho_WJcgl8C"
   },
   "source": [
    "The best performing model was BiLSTM with 2 layers and size 20: Accuracy - 0.5776\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0w92d8FwiD5"
   },
   "source": [
    "**Of all the RNN Models the best performing model was LSTM with 1 layers and size 20: Accuracy - 0.5982**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0M2kmz2hu65O"
   },
   "source": [
    "## CONCLUSION\n",
    "\n",
    "**The best perfoming model turned out to be the  word embeddings word2vec model using genism with size=200, window=7, min_count= 2,5 and Accuracy: 0.6053**\n",
    "\n",
    "I took 2.5% of the entire dataset to run all the models. Potentially with better hyperparameter tuning running more epochs and taking larger chunck of data the model performance could be improved a lot.\n",
    "\n",
    "Working with unstructured textual data is tricky and the experiments provided insights into the performance trade-offs, the impact of model complexity, and the significance of leveraging pre-trained embeddings and models.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
